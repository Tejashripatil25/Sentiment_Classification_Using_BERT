### Sentiment_Classification_Using_BERT

BERT stands for Bidirectional Representation for Transformers, was proposed by researchers at Google AI language in 2018.

The main aim of that was to improve the understanding of the meaning of queries related to Google Search, 

BERT becomes one of the most important and complete architecture for various natural language tasks having generated state-of-the-art results on Sentence pair classification task, question-answer task, etc.

### Architecture:

One of the most important features of BERT is that its adaptability to perform different NLP tasks with state-of-the-art accuracy (similar to the transfer learning we used in Computer vision). For that, the paper also proposed the architecture of different tasks.

We will be using BERT architecture for single sentence classification tasks specifically the architecture used for CoLA (Corpus of Linguistic Acceptability) binary classification task. 

![image](https://github.com/Tejashripatil25/Sentiment_Classification_Using_BERT/assets/124791646/a230d216-0548-4fbf-8954-f02529b65a88)

BERT has proposed in the two versions:

#### BERT (BASE): 12 layers of encoder stack with 12 bidirectional self-attention heads and 768 hidden units.

#### BERT (LARGE): 24 layers of encoder stack with 24 bidirectional self-attention heads and 1024 hidden units.

